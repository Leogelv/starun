import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';
import { OpenAIError } from '@/types/openai';

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

export async function POST(request: NextRequest) {
  try {
    console.log('=== Speech API POST called ===');
    
    const formData = await request.formData();
    console.log('FormData entries:', Array.from(formData.entries()).map(([key, value]) => ({
      key,
      valueType: typeof value,
      isFile: value instanceof File,
      ...(value instanceof File && { name: value.name, size: value.size, type: value.type })
    })));
    
    const audioFile = formData.get('audio') as File;
    
    if (!audioFile) {
      console.error('âŒ No audio file provided in FormData');
      return NextResponse.json({ 
        error: 'No audio file provided',
        success: false,
        text: 'ÐÑƒÐ´Ð¸Ð¾ Ñ„Ð°Ð¹Ð» Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½ Ð² Ð·Ð°Ð¿Ñ€Ð¾ÑÐµ'
      }, { status: 400 });
    }

    console.log('âœ… Received audio file:', {
      name: audioFile.name,
      type: audioFile.type,
      size: audioFile.size
    });

    // Validate file size (OpenAI Whisper has 25MB limit)
    const maxSize = 25 * 1024 * 1024; // 25MB
    if (audioFile.size > maxSize) {
      console.error('âŒ File too large:', audioFile.size, 'bytes (max:', maxSize, ')');
      return NextResponse.json({ 
        error: 'File too large',
        success: false,
        text: 'ÐÑƒÐ´Ð¸Ð¾ Ñ„Ð°Ð¹Ð» ÑÐ»Ð¸ÑˆÐºÐ¾Ð¼ Ð±Ð¾Ð»ÑŒÑˆÐ¾Ð¹. ÐœÐ°ÐºÑÐ¸Ð¼Ð°Ð»ÑŒÐ½Ñ‹Ð¹ Ñ€Ð°Ð·Ð¼ÐµÑ€: 25MB'
      }, { status: 400 });
    }

    // Check if file is empty
    if (audioFile.size === 0) {
      console.error('âŒ Empty file');
      return NextResponse.json({ 
        error: 'Empty file',
        success: false,
        text: 'ÐÑƒÐ´Ð¸Ð¾ Ñ„Ð°Ð¹Ð» Ð¿ÑƒÑÑ‚Ð¾Ð¹'
      }, { status: 400 });
    }

    // Check if OpenAI API key is configured
    if (!process.env.OPENAI_API_KEY) {
      console.error('OpenAI API key not configured');
      return NextResponse.json({ 
        text: 'OpenAI API ÐºÐ»ÑŽÑ‡ Ð½Ðµ Ð½Ð°ÑÑ‚Ñ€Ð¾ÐµÐ½. ÐŸÑ€Ð¾Ð²ÐµÑ€ÑŒÑ‚Ðµ Ð¿ÐµÑ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¾ÐºÑ€ÑƒÐ¶ÐµÐ½Ð¸Ñ.',
        success: false,
        error: 'API key missing'
      });
    }

    // Convert and process audio file for OpenAI Whisper
    try {
      // Determine proper file extension and type
      let fileName = audioFile.name;
      let mimeType = audioFile.type;
      
      // Handle different audio formats
      if (fileName.endsWith('.m4a') || mimeType === 'audio/m4a') {
        fileName = fileName.endsWith('.m4a') ? fileName : 'recording.m4a';
        mimeType = 'audio/m4a';
      } else if (fileName.endsWith('.webm') || mimeType.includes('webm')) {
        fileName = fileName.endsWith('.webm') ? fileName : 'recording.webm';
        mimeType = 'audio/webm';
      } else if (fileName.endsWith('.wav') || mimeType === 'audio/wav') {
        fileName = fileName.endsWith('.wav') ? fileName : 'recording.wav';
        mimeType = 'audio/wav';
      } else if (fileName.endsWith('.mp3') || mimeType === 'audio/mp3') {
        fileName = fileName.endsWith('.mp3') ? fileName : 'recording.mp3';
        mimeType = 'audio/mp3';
      } else {
        // Default to webm if unknown
        fileName = 'recording.webm';
        mimeType = 'audio/webm';
      }
      
      // Convert File to buffer - OpenAI SDK handles the rest
      const buffer = Buffer.from(await audioFile.arrayBuffer());
      
      // Create a blob-like object that OpenAI SDK expects
      const fileBlob = new Blob([buffer], { type: mimeType });
      Object.defineProperty(fileBlob, 'name', { value: fileName });

      console.log('âœ… Sending to OpenAI Whisper:', {
        fileName,
        type: mimeType,
        size: buffer.length
      });
      
      console.log('ðŸ“¤ Making request to OpenAI Whisper...');
      const transcription = await openai.audio.transcriptions.create({
        file: fileBlob as any,
        model: 'whisper-1',
        language: 'ru',
        response_format: 'verbose_json',
        temperature: 0.0,
        prompt: 'Ð­Ñ‚Ð¾ Ð·Ð°Ð¿Ñ€Ð¾Ñ Ð¾ Ð¼ÐµÐ´Ð¸Ñ‚Ð°Ñ†Ð¸Ð¸, Ð¹Ð¾Ð³Ðµ, Ð¿Ñ€Ð°ÐºÑ‚Ð¸ÐºÐ°Ñ… Ð¾ÑÐ¾Ð·Ð½Ð°Ð½Ð½Ð¾ÑÑ‚Ð¸ Ð¸Ð»Ð¸ Ð·Ð´Ð¾Ñ€Ð¾Ð²ÑŒÐµ.',
      });

      console.log('âœ… Transcription successful:', {
        text: transcription.text,
        language: transcription.language,
        duration: transcription.duration,
        segments: transcription.segments?.length
      });

      const transcribedText = transcription.text || '';
      const confidence = transcription.segments?.[0]?.avg_logprob || -1;
      
      console.log('Transcription confidence:', confidence);
      
      if (!transcribedText || transcribedText.trim() === '') {
        return NextResponse.json({ 
          text: 'Ð ÐµÑ‡ÑŒ Ð½Ðµ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð½Ð°. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ñ‚ÑŒ Ð³Ñ€Ð¾Ð¼Ñ‡Ðµ Ð¸ Ñ‡ÐµÑ‚Ñ‡Ðµ.',
          success: false
        });
      }
      
      // Check for common hallucinations
      const commonHallucinations = [
        'ÑÐ¿Ð°ÑÐ¸Ð±Ð¾ Ð·Ð° Ð¿Ñ€Ð¾ÑÐ¼Ð¾Ñ‚Ñ€',
        'ÑÐ¿Ð°ÑÐ¸Ð±Ð¾ Ð·Ð° Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ',
        'Ð¿Ð¾Ð´Ð¿Ð¸ÑÑ‹Ð²Ð°Ð¹Ñ‚ÐµÑÑŒ Ð½Ð° ÐºÐ°Ð½Ð°Ð»',
        'like and subscribe',
        'Ð¼ÑƒÐ·Ñ‹ÐºÐ°',
        'ÑÑƒÐ±Ñ‚Ð¸Ñ‚Ñ€Ñ‹',
        'Ð¿ÐµÑ€ÐµÐ²Ð¾Ð´Ñ‡Ð¸Ðº',
        'Thank you'
      ];
      
      const isLikelyHallucination = commonHallucinations.some(phrase => 
        transcribedText.toLowerCase().includes(phrase.toLowerCase())
      );
      
      if (isLikelyHallucination || confidence < -0.8) {
        return NextResponse.json({ 
          text: 'ÐÐµ ÑƒÐ´Ð°Ð»Ð¾ÑÑŒ Ñ‡ÐµÑ‚ÐºÐ¾ Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ñ‚ÑŒ Ñ€ÐµÑ‡ÑŒ. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÑÐºÐ°Ð·Ð°Ñ‚ÑŒ ÐµÑ‰Ðµ Ñ€Ð°Ð· Ð±Ð¾Ð»ÐµÐµ Ñ‡ÐµÑ‚ÐºÐ¾.',
          success: false,
          debug: { confidence, text: transcribedText }
        });
      }

      return NextResponse.json({ 
        text: transcribedText.trim(),
        success: true,
        confidence 
      });

    } catch (openaiError: unknown) {
      const error = openaiError as OpenAIError;
      console.error('OpenAI Whisper detailed error:', {
        message: error.message,
        type: error.type,
        code: error.code,
        status: error.status,
        stack: error.stack
      });
      
      let errorMessage = 'ÐžÑˆÐ¸Ð±ÐºÐ° Ñ€Ð°ÑÐ¿Ð¾Ð·Ð½Ð°Ð²Ð°Ð½Ð¸Ñ Ñ€ÐµÑ‡Ð¸.';
      
      if (error.code === 'invalid_api_key') {
        errorMessage = 'ÐÐµÐ²ÐµÑ€Ð½Ñ‹Ð¹ API ÐºÐ»ÑŽÑ‡ OpenAI.';
      } else if (error.code === 'insufficient_quota') {
        errorMessage = 'ÐŸÑ€ÐµÐ²Ñ‹ÑˆÐµÐ½Ð° ÐºÐ²Ð¾Ñ‚Ð° OpenAI API.';
      } else if (error.message?.includes('audio')) {
        errorMessage = 'ÐÐµÐ¿Ð¾Ð´Ð´ÐµÑ€Ð¶Ð¸Ð²Ð°ÐµÐ¼Ñ‹Ð¹ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚ Ð°ÑƒÐ´Ð¸Ð¾. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ ÐµÑ‰Ðµ Ñ€Ð°Ð·.';
      }
      
      return NextResponse.json({ 
        text: errorMessage,
        success: false,
        error: error.message,
        code: error.code
      });
    }

  } catch (error: unknown) {
    const generalError = error as Error;
    console.error('Speech transcription general error:', error);
    return NextResponse.json(
      { 
        error: 'Failed to transcribe audio', 
        details: generalError.message,
        text: 'ÐžÐ±Ñ‰Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° ÑÐµÑ€Ð²ÐµÑ€Ð°. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¿Ð¾Ð·Ð¶Ðµ.',
        success: false
      },
      { status: 500 }
    );
  }
} 